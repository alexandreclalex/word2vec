{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from CONFIG import PREPROCESSED_DATA_PATH\n",
    "from knn import KNN\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "from classifier import Classifier\n",
    "from word2vec import Word2VecModel\n",
    "import math\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some configs\n",
    "SHOULD_RETRAIN_WORD2VEC = True\n",
    "WORD2VEC_PATH = \"custom_word_2_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.19544457292888"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(70**2 + 60**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and shuffle\n",
    "df_data = pd.read_parquet(PREPROCESSED_DATA_PATH)\n",
    "df_data = df_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "9/9 - 1s - loss: 0.6926\n",
      "Epoch 2/1024\n",
      "9/9 - 1s - loss: 0.6915\n",
      "Epoch 3/1024\n",
      "9/9 - 1s - loss: 0.6899\n",
      "Epoch 4/1024\n",
      "9/9 - 1s - loss: 0.6878\n",
      "Epoch 5/1024\n",
      "9/9 - 1s - loss: 0.6850\n",
      "Epoch 6/1024\n",
      "9/9 - 1s - loss: 0.6812\n",
      "Epoch 7/1024\n",
      "9/9 - 1s - loss: 0.6767\n",
      "Epoch 8/1024\n",
      "9/9 - 1s - loss: 0.6716\n",
      "Epoch 9/1024\n",
      "9/9 - 1s - loss: 0.6655\n",
      "Epoch 10/1024\n",
      "9/9 - 1s - loss: 0.6581\n",
      "Epoch 11/1024\n",
      "9/9 - 1s - loss: 0.6514\n",
      "Epoch 12/1024\n",
      "9/9 - 1s - loss: 0.6437\n",
      "Epoch 13/1024\n",
      "9/9 - 1s - loss: 0.6352\n",
      "Epoch 14/1024\n",
      "9/9 - 1s - loss: 0.6267\n",
      "Epoch 15/1024\n",
      "9/9 - 1s - loss: 0.6201\n",
      "Epoch 16/1024\n",
      "9/9 - 1s - loss: 0.6109\n",
      "Epoch 17/1024\n",
      "9/9 - 1s - loss: 0.6026\n",
      "Epoch 18/1024\n",
      "9/9 - 1s - loss: 0.5955\n",
      "Epoch 19/1024\n",
      "9/9 - 1s - loss: 0.5874\n",
      "Epoch 20/1024\n",
      "9/9 - 1s - loss: 0.5792\n",
      "Epoch 21/1024\n",
      "9/9 - 1s - loss: 0.5737\n",
      "Epoch 22/1024\n",
      "9/9 - 1s - loss: 0.5672\n",
      "Epoch 23/1024\n",
      "9/9 - 1s - loss: 0.5604\n",
      "Epoch 24/1024\n",
      "9/9 - 1s - loss: 0.5549\n",
      "Epoch 25/1024\n",
      "9/9 - 1s - loss: 0.5518\n",
      "Epoch 26/1024\n",
      "9/9 - 1s - loss: 0.5427\n",
      "Epoch 27/1024\n",
      "9/9 - 1s - loss: 0.5425\n",
      "Epoch 28/1024\n",
      "9/9 - 1s - loss: 0.5395\n",
      "Epoch 29/1024\n",
      "9/9 - 1s - loss: 0.5331\n",
      "Epoch 30/1024\n",
      "9/9 - 1s - loss: 0.5306\n",
      "Epoch 31/1024\n",
      "9/9 - 1s - loss: 0.5299\n",
      "Epoch 32/1024\n",
      "9/9 - 1s - loss: 0.5260\n",
      "Epoch 33/1024\n",
      "9/9 - 1s - loss: 0.5240\n",
      "Epoch 34/1024\n",
      "9/9 - 1s - loss: 0.5194\n",
      "Epoch 35/1024\n",
      "9/9 - 1s - loss: 0.5186\n",
      "Epoch 36/1024\n",
      "9/9 - 1s - loss: 0.5222\n",
      "Epoch 37/1024\n",
      "9/9 - 1s - loss: 0.5174\n",
      "Epoch 38/1024\n",
      "9/9 - 1s - loss: 0.5182\n",
      "Epoch 39/1024\n",
      "9/9 - 1s - loss: 0.5122\n",
      "Epoch 40/1024\n",
      "9/9 - 1s - loss: 0.5123\n",
      "Epoch 41/1024\n",
      "9/9 - 1s - loss: 0.5100\n",
      "Epoch 42/1024\n",
      "9/9 - 1s - loss: 0.5083\n",
      "Epoch 43/1024\n",
      "9/9 - 1s - loss: 0.5057\n",
      "Epoch 44/1024\n",
      "9/9 - 1s - loss: 0.5062\n",
      "Epoch 45/1024\n",
      "9/9 - 1s - loss: 0.5030\n",
      "Epoch 46/1024\n",
      "9/9 - 1s - loss: 0.5016\n",
      "Epoch 47/1024\n",
      "9/9 - 1s - loss: 0.5039\n",
      "Epoch 48/1024\n",
      "9/9 - 1s - loss: 0.4998\n",
      "Epoch 49/1024\n",
      "9/9 - 1s - loss: 0.5017\n",
      "Epoch 50/1024\n",
      "9/9 - 1s - loss: 0.4997\n",
      "Epoch 51/1024\n",
      "9/9 - 1s - loss: 0.5001\n",
      "Epoch 52/1024\n",
      "9/9 - 1s - loss: 0.4972\n",
      "Epoch 53/1024\n",
      "9/9 - 1s - loss: 0.4945\n",
      "Epoch 54/1024\n",
      "9/9 - 1s - loss: 0.4954\n",
      "Epoch 55/1024\n",
      "9/9 - 1s - loss: 0.4965\n",
      "Epoch 56/1024\n",
      "9/9 - 1s - loss: 0.4960\n",
      "Epoch 57/1024\n",
      "9/9 - 1s - loss: 0.4916\n",
      "Epoch 58/1024\n",
      "9/9 - 1s - loss: 0.4927\n",
      "Epoch 59/1024\n",
      "9/9 - 1s - loss: 0.4931\n",
      "Epoch 60/1024\n",
      "9/9 - 1s - loss: 0.4907\n",
      "Epoch 61/1024\n",
      "9/9 - 1s - loss: 0.4891\n",
      "Epoch 62/1024\n",
      "9/9 - 1s - loss: 0.4908\n",
      "Epoch 63/1024\n",
      "9/9 - 1s - loss: 0.4857\n",
      "Epoch 64/1024\n",
      "9/9 - 1s - loss: 0.4859\n",
      "Epoch 65/1024\n",
      "9/9 - 1s - loss: 0.4914\n",
      "Epoch 66/1024\n",
      "9/9 - 1s - loss: 0.4865\n",
      "Epoch 67/1024\n",
      "9/9 - 1s - loss: 0.4859\n",
      "Epoch 68/1024\n",
      "9/9 - 1s - loss: 0.4872\n",
      "Epoch 69/1024\n",
      "9/9 - 1s - loss: 0.4839\n",
      "Epoch 70/1024\n",
      "9/9 - 1s - loss: 0.4819\n",
      "Epoch 71/1024\n",
      "9/9 - 1s - loss: 0.4841\n",
      "Epoch 72/1024\n",
      "9/9 - 1s - loss: 0.4853\n",
      "Epoch 73/1024\n",
      "9/9 - 1s - loss: 0.4826\n",
      "Epoch 74/1024\n",
      "9/9 - 1s - loss: 0.4807\n",
      "Epoch 75/1024\n",
      "9/9 - 1s - loss: 0.4788\n",
      "Epoch 76/1024\n",
      "9/9 - 1s - loss: 0.4802\n",
      "Epoch 77/1024\n",
      "9/9 - 1s - loss: 0.4802\n",
      "Epoch 78/1024\n",
      "9/9 - 1s - loss: 0.4802\n",
      "Epoch 79/1024\n",
      "9/9 - 1s - loss: 0.4787\n",
      "Epoch 80/1024\n",
      "9/9 - 1s - loss: 0.4781\n",
      "Epoch 81/1024\n",
      "9/9 - 1s - loss: 0.4784\n",
      "Epoch 82/1024\n",
      "9/9 - 1s - loss: 0.4779\n",
      "Epoch 83/1024\n",
      "9/9 - 1s - loss: 0.4765\n",
      "Epoch 84/1024\n",
      "9/9 - 1s - loss: 0.4751\n",
      "Epoch 85/1024\n",
      "9/9 - 1s - loss: 0.4777\n",
      "Epoch 86/1024\n",
      "9/9 - 1s - loss: 0.4775\n",
      "Epoch 87/1024\n",
      "9/9 - 1s - loss: 0.4725\n",
      "Epoch 88/1024\n",
      "9/9 - 1s - loss: 0.4727\n",
      "Epoch 89/1024\n",
      "9/9 - 1s - loss: 0.4755\n",
      "Epoch 90/1024\n",
      "9/9 - 1s - loss: 0.4756\n",
      "Epoch 91/1024\n",
      "9/9 - 1s - loss: 0.4762\n",
      "Epoch 92/1024\n",
      "9/9 - 1s - loss: 0.4733\n",
      "Epoch 93/1024\n",
      "9/9 - 1s - loss: 0.4712\n",
      "Epoch 94/1024\n",
      "9/9 - 1s - loss: 0.4730\n",
      "Epoch 95/1024\n",
      "9/9 - 1s - loss: 0.4708\n",
      "Epoch 96/1024\n",
      "9/9 - 1s - loss: 0.4731\n",
      "Epoch 97/1024\n",
      "9/9 - 1s - loss: 0.4693\n",
      "Epoch 98/1024\n",
      "9/9 - 1s - loss: 0.4690\n",
      "Epoch 99/1024\n",
      "9/9 - 1s - loss: 0.4712\n",
      "Epoch 100/1024\n",
      "9/9 - 1s - loss: 0.4701\n",
      "Epoch 101/1024\n",
      "9/9 - 1s - loss: 0.4668\n",
      "Epoch 102/1024\n",
      "9/9 - 1s - loss: 0.4698\n",
      "Epoch 103/1024\n",
      "9/9 - 1s - loss: 0.4698\n",
      "Epoch 104/1024\n",
      "9/9 - 1s - loss: 0.4662\n",
      "Epoch 105/1024\n",
      "9/9 - 1s - loss: 0.4657\n",
      "Epoch 106/1024\n",
      "9/9 - 1s - loss: 0.4652\n",
      "Epoch 107/1024\n",
      "9/9 - 1s - loss: 0.4674\n",
      "Epoch 108/1024\n",
      "9/9 - 1s - loss: 0.4656\n",
      "Epoch 109/1024\n",
      "9/9 - 1s - loss: 0.4687\n",
      "Epoch 110/1024\n",
      "9/9 - 1s - loss: 0.4643\n",
      "Epoch 111/1024\n",
      "9/9 - 1s - loss: 0.4681\n",
      "Epoch 112/1024\n",
      "9/9 - 1s - loss: 0.4611\n",
      "Epoch 113/1024\n",
      "9/9 - 1s - loss: 0.4628\n",
      "Epoch 114/1024\n",
      "9/9 - 1s - loss: 0.4647\n",
      "Epoch 115/1024\n",
      "9/9 - 1s - loss: 0.4646\n",
      "Epoch 116/1024\n",
      "9/9 - 1s - loss: 0.4575\n",
      "Epoch 117/1024\n",
      "9/9 - 1s - loss: 0.4666\n",
      "Epoch 118/1024\n",
      "9/9 - 1s - loss: 0.4621\n",
      "Epoch 119/1024\n",
      "9/9 - 1s - loss: 0.4615\n",
      "Epoch 120/1024\n",
      "9/9 - 1s - loss: 0.4613\n",
      "Epoch 121/1024\n",
      "9/9 - 1s - loss: 0.4593\n",
      "Epoch 122/1024\n",
      "9/9 - 1s - loss: 0.4584\n",
      "Epoch 123/1024\n",
      "9/9 - 1s - loss: 0.4601\n",
      "Epoch 124/1024\n",
      "9/9 - 1s - loss: 0.4620\n",
      "Epoch 125/1024\n",
      "9/9 - 1s - loss: 0.4588\n",
      "Epoch 126/1024\n",
      "9/9 - 1s - loss: 0.4588\n",
      "Epoch 127/1024\n",
      "9/9 - 1s - loss: 0.4571\n",
      "Epoch 128/1024\n",
      "9/9 - 1s - loss: 0.4593\n",
      "Epoch 129/1024\n",
      "9/9 - 1s - loss: 0.4607\n",
      "Epoch 130/1024\n",
      "9/9 - 1s - loss: 0.4571\n",
      "Epoch 131/1024\n",
      "9/9 - 1s - loss: 0.4581\n",
      "Epoch 132/1024\n",
      "9/9 - 1s - loss: 0.4576\n",
      "Epoch 133/1024\n",
      "9/9 - 1s - loss: 0.4552\n",
      "Epoch 134/1024\n",
      "9/9 - 1s - loss: 0.4532\n",
      "Epoch 135/1024\n",
      "9/9 - 1s - loss: 0.4561\n",
      "Epoch 136/1024\n",
      "9/9 - 1s - loss: 0.4548\n",
      "Epoch 137/1024\n",
      "9/9 - 1s - loss: 0.4531\n",
      "Epoch 138/1024\n",
      "9/9 - 1s - loss: 0.4543\n",
      "Epoch 139/1024\n",
      "9/9 - 1s - loss: 0.4531\n",
      "Epoch 140/1024\n",
      "9/9 - 1s - loss: 0.4556\n",
      "Epoch 141/1024\n",
      "9/9 - 1s - loss: 0.4500\n",
      "Epoch 142/1024\n",
      "9/9 - 1s - loss: 0.4515\n",
      "Epoch 143/1024\n",
      "9/9 - 1s - loss: 0.4526\n",
      "Epoch 144/1024\n",
      "9/9 - 1s - loss: 0.4524\n",
      "Epoch 145/1024\n",
      "9/9 - 1s - loss: 0.4510\n",
      "Epoch 146/1024\n",
      "9/9 - 1s - loss: 0.4542\n",
      "Epoch 147/1024\n",
      "9/9 - 1s - loss: 0.4505\n",
      "Epoch 148/1024\n",
      "9/9 - 1s - loss: 0.4543\n",
      "Epoch 149/1024\n",
      "9/9 - 1s - loss: 0.4507\n",
      "Epoch 150/1024\n",
      "9/9 - 1s - loss: 0.4496\n",
      "Epoch 151/1024\n",
      "9/9 - 1s - loss: 0.4480\n",
      "Epoch 152/1024\n",
      "9/9 - 1s - loss: 0.4500\n",
      "Epoch 153/1024\n",
      "9/9 - 1s - loss: 0.4522\n",
      "Epoch 154/1024\n",
      "9/9 - 1s - loss: 0.4491\n",
      "Epoch 155/1024\n",
      "9/9 - 1s - loss: 0.4490\n",
      "Epoch 156/1024\n",
      "9/9 - 1s - loss: 0.4442\n",
      "Epoch 157/1024\n",
      "9/9 - 1s - loss: 0.4492\n",
      "Epoch 158/1024\n",
      "9/9 - 1s - loss: 0.4448\n",
      "Epoch 159/1024\n",
      "9/9 - 1s - loss: 0.4467\n",
      "Epoch 160/1024\n",
      "9/9 - 1s - loss: 0.4461\n",
      "Epoch 161/1024\n",
      "9/9 - 1s - loss: 0.4496\n",
      "Epoch 162/1024\n",
      "9/9 - 1s - loss: 0.4506\n",
      "Epoch 163/1024\n",
      "9/9 - 1s - loss: 0.4418\n",
      "Epoch 164/1024\n",
      "9/9 - 1s - loss: 0.4441\n",
      "Epoch 165/1024\n",
      "9/9 - 1s - loss: 0.4432\n",
      "Epoch 166/1024\n",
      "9/9 - 1s - loss: 0.4457\n",
      "Epoch 167/1024\n",
      "9/9 - 1s - loss: 0.4453\n",
      "Epoch 168/1024\n",
      "9/9 - 1s - loss: 0.4440\n",
      "Epoch 169/1024\n",
      "9/9 - 1s - loss: 0.4446\n",
      "Epoch 170/1024\n",
      "9/9 - 1s - loss: 0.4454\n",
      "Epoch 171/1024\n",
      "9/9 - 1s - loss: 0.4458\n",
      "Epoch 172/1024\n",
      "9/9 - 1s - loss: 0.4430\n",
      "Epoch 173/1024\n",
      "9/9 - 1s - loss: 0.4437\n",
      "Epoch 174/1024\n",
      "9/9 - 1s - loss: 0.4449\n",
      "Epoch 175/1024\n",
      "9/9 - 1s - loss: 0.4419\n",
      "Epoch 176/1024\n",
      "9/9 - 1s - loss: 0.4456\n",
      "Epoch 177/1024\n",
      "9/9 - 1s - loss: 0.4440\n",
      "Epoch 178/1024\n",
      "9/9 - 1s - loss: 0.4433\n",
      "Epoch 179/1024\n",
      "9/9 - 1s - loss: 0.4394\n",
      "Epoch 180/1024\n",
      "9/9 - 1s - loss: 0.4400\n",
      "Epoch 181/1024\n",
      "9/9 - 1s - loss: 0.4398\n",
      "Epoch 182/1024\n",
      "9/9 - 1s - loss: 0.4341\n",
      "Epoch 183/1024\n",
      "9/9 - 1s - loss: 0.4417\n",
      "Epoch 184/1024\n",
      "9/9 - 1s - loss: 0.4411\n",
      "Epoch 185/1024\n",
      "9/9 - 1s - loss: 0.4419\n",
      "Epoch 186/1024\n",
      "9/9 - 1s - loss: 0.4457\n",
      "Epoch 187/1024\n",
      "9/9 - 1s - loss: 0.4421\n",
      "Epoch 188/1024\n",
      "9/9 - 1s - loss: 0.4381\n",
      "Epoch 189/1024\n",
      "9/9 - 1s - loss: 0.4402\n",
      "Epoch 190/1024\n",
      "9/9 - 1s - loss: 0.4437\n",
      "Epoch 191/1024\n",
      "9/9 - 1s - loss: 0.4381\n",
      "Epoch 192/1024\n",
      "9/9 - 1s - loss: 0.4379\n",
      "Epoch 193/1024\n",
      "9/9 - 1s - loss: 0.4401\n",
      "Epoch 194/1024\n",
      "9/9 - 1s - loss: 0.4333\n",
      "Epoch 195/1024\n",
      "9/9 - 1s - loss: 0.4379\n",
      "Epoch 196/1024\n",
      "9/9 - 1s - loss: 0.4355\n",
      "Epoch 197/1024\n",
      "9/9 - 1s - loss: 0.4382\n",
      "Epoch 198/1024\n",
      "9/9 - 1s - loss: 0.4339\n",
      "Epoch 199/1024\n",
      "9/9 - 1s - loss: 0.4348\n",
      "Epoch 200/1024\n",
      "9/9 - 1s - loss: 0.4359\n",
      "Epoch 201/1024\n",
      "9/9 - 1s - loss: 0.4365\n",
      "Epoch 202/1024\n",
      "9/9 - 1s - loss: 0.4363\n",
      "Epoch 203/1024\n",
      "9/9 - 1s - loss: 0.4372\n",
      "Epoch 204/1024\n",
      "9/9 - 1s - loss: 0.4318\n",
      "Epoch 205/1024\n",
      "9/9 - 1s - loss: 0.4363\n",
      "Epoch 206/1024\n",
      "9/9 - 1s - loss: 0.4368\n",
      "Epoch 207/1024\n",
      "9/9 - 1s - loss: 0.4333\n",
      "Epoch 208/1024\n",
      "9/9 - 1s - loss: 0.4332\n",
      "Epoch 209/1024\n",
      "9/9 - 1s - loss: 0.4302\n",
      "Epoch 210/1024\n",
      "9/9 - 1s - loss: 0.4381\n",
      "Epoch 211/1024\n",
      "9/9 - 1s - loss: 0.4309\n",
      "Epoch 212/1024\n",
      "9/9 - 1s - loss: 0.4316\n",
      "Epoch 213/1024\n",
      "9/9 - 1s - loss: 0.4360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/1024\n",
      "9/9 - 1s - loss: 0.4326\n",
      "Epoch 215/1024\n",
      "9/9 - 1s - loss: 0.4315\n",
      "Epoch 216/1024\n",
      "9/9 - 1s - loss: 0.4300\n",
      "Epoch 217/1024\n",
      "9/9 - 1s - loss: 0.4352\n",
      "Epoch 218/1024\n",
      "9/9 - 1s - loss: 0.4329\n",
      "Epoch 219/1024\n",
      "9/9 - 1s - loss: 0.4289\n",
      "Epoch 220/1024\n",
      "9/9 - 1s - loss: 0.4296\n",
      "Epoch 221/1024\n",
      "9/9 - 1s - loss: 0.4328\n",
      "Epoch 222/1024\n",
      "9/9 - 1s - loss: 0.4302\n",
      "Epoch 223/1024\n",
      "9/9 - 1s - loss: 0.4319\n",
      "Epoch 224/1024\n",
      "9/9 - 1s - loss: 0.4324\n",
      "Epoch 225/1024\n",
      "9/9 - 1s - loss: 0.4336\n",
      "Epoch 226/1024\n",
      "9/9 - 1s - loss: 0.4300\n",
      "Epoch 227/1024\n",
      "9/9 - 1s - loss: 0.4324\n",
      "Epoch 228/1024\n",
      "9/9 - 1s - loss: 0.4314\n",
      "Epoch 229/1024\n",
      "9/9 - 1s - loss: 0.4270\n",
      "Epoch 230/1024\n",
      "9/9 - 1s - loss: 0.4303\n",
      "Epoch 231/1024\n",
      "9/9 - 1s - loss: 0.4332\n",
      "Epoch 232/1024\n",
      "9/9 - 1s - loss: 0.4274\n",
      "Epoch 233/1024\n",
      "9/9 - 1s - loss: 0.4260\n",
      "Epoch 234/1024\n",
      "9/9 - 1s - loss: 0.4279\n",
      "Epoch 235/1024\n",
      "9/9 - 1s - loss: 0.4269\n",
      "Epoch 236/1024\n",
      "9/9 - 1s - loss: 0.4295\n",
      "Epoch 237/1024\n",
      "9/9 - 1s - loss: 0.4271\n",
      "Epoch 238/1024\n",
      "9/9 - 1s - loss: 0.4229\n",
      "Epoch 239/1024\n",
      "9/9 - 1s - loss: 0.4285\n",
      "Epoch 240/1024\n",
      "9/9 - 1s - loss: 0.4232\n",
      "Epoch 241/1024\n",
      "9/9 - 1s - loss: 0.4219\n",
      "Epoch 242/1024\n",
      "9/9 - 1s - loss: 0.4257\n",
      "Epoch 243/1024\n",
      "9/9 - 1s - loss: 0.4260\n",
      "Epoch 244/1024\n",
      "9/9 - 1s - loss: 0.4267\n",
      "Epoch 245/1024\n",
      "9/9 - 1s - loss: 0.4225\n",
      "Epoch 246/1024\n",
      "9/9 - 1s - loss: 0.4212\n",
      "Epoch 247/1024\n",
      "9/9 - 1s - loss: 0.4238\n",
      "Epoch 248/1024\n",
      "9/9 - 1s - loss: 0.4195\n",
      "Epoch 249/1024\n",
      "9/9 - 1s - loss: 0.4261\n",
      "Epoch 250/1024\n",
      "9/9 - 1s - loss: 0.4263\n",
      "Epoch 251/1024\n",
      "9/9 - 1s - loss: 0.4238\n",
      "Epoch 252/1024\n",
      "9/9 - 1s - loss: 0.4250\n",
      "Epoch 253/1024\n",
      "9/9 - 1s - loss: 0.4222\n",
      "Epoch 254/1024\n",
      "9/9 - 1s - loss: 0.4223\n",
      "Epoch 255/1024\n",
      "9/9 - 1s - loss: 0.4205\n",
      "Epoch 256/1024\n",
      "9/9 - 1s - loss: 0.4231\n",
      "Epoch 257/1024\n",
      "9/9 - 1s - loss: 0.4204\n",
      "Epoch 258/1024\n",
      "9/9 - 1s - loss: 0.4232\n",
      "Epoch 259/1024\n",
      "9/9 - 1s - loss: 0.4217\n",
      "Epoch 260/1024\n",
      "9/9 - 1s - loss: 0.4204\n",
      "Epoch 261/1024\n",
      "9/9 - 1s - loss: 0.4220\n",
      "Epoch 262/1024\n",
      "9/9 - 1s - loss: 0.4206\n",
      "Epoch 263/1024\n",
      "9/9 - 1s - loss: 0.4218\n",
      "Epoch 264/1024\n",
      "9/9 - 1s - loss: 0.4181\n",
      "Epoch 265/1024\n",
      "9/9 - 1s - loss: 0.4216\n",
      "Epoch 266/1024\n",
      "9/9 - 1s - loss: 0.4213\n",
      "Epoch 267/1024\n",
      "9/9 - 1s - loss: 0.4182\n",
      "Epoch 268/1024\n",
      "9/9 - 1s - loss: 0.4168\n",
      "Epoch 269/1024\n",
      "9/9 - 1s - loss: 0.4184\n",
      "Epoch 270/1024\n",
      "9/9 - 1s - loss: 0.4203\n",
      "Epoch 271/1024\n",
      "9/9 - 1s - loss: 0.4167\n",
      "Epoch 272/1024\n",
      "9/9 - 1s - loss: 0.4181\n",
      "Epoch 273/1024\n",
      "9/9 - 1s - loss: 0.4181\n",
      "Epoch 274/1024\n",
      "9/9 - 1s - loss: 0.4177\n",
      "Epoch 275/1024\n",
      "9/9 - 1s - loss: 0.4167\n",
      "Epoch 276/1024\n",
      "9/9 - 1s - loss: 0.4156\n",
      "Epoch 277/1024\n",
      "9/9 - 1s - loss: 0.4158\n",
      "Epoch 278/1024\n",
      "9/9 - 1s - loss: 0.4187\n",
      "Epoch 279/1024\n",
      "9/9 - 1s - loss: 0.4147\n",
      "Epoch 280/1024\n",
      "9/9 - 1s - loss: 0.4160\n",
      "Epoch 281/1024\n",
      "9/9 - 1s - loss: 0.4199\n",
      "Epoch 282/1024\n",
      "9/9 - 1s - loss: 0.4136\n",
      "Epoch 283/1024\n",
      "9/9 - 1s - loss: 0.4156\n",
      "Epoch 284/1024\n",
      "9/9 - 1s - loss: 0.4149\n",
      "Epoch 285/1024\n",
      "9/9 - 1s - loss: 0.4133\n",
      "Epoch 286/1024\n",
      "9/9 - 1s - loss: 0.4174\n",
      "Epoch 287/1024\n",
      "9/9 - 1s - loss: 0.4130\n",
      "Epoch 288/1024\n",
      "9/9 - 1s - loss: 0.4104\n",
      "Epoch 289/1024\n",
      "9/9 - 1s - loss: 0.4150\n",
      "Epoch 290/1024\n",
      "9/9 - 1s - loss: 0.4130\n",
      "Epoch 291/1024\n",
      "9/9 - 1s - loss: 0.4155\n",
      "Epoch 292/1024\n",
      "9/9 - 1s - loss: 0.4094\n",
      "Epoch 293/1024\n",
      "9/9 - 1s - loss: 0.4139\n",
      "Epoch 294/1024\n",
      "9/9 - 1s - loss: 0.4142\n",
      "Epoch 295/1024\n",
      "9/9 - 1s - loss: 0.4131\n",
      "Epoch 296/1024\n",
      "9/9 - 1s - loss: 0.4145\n",
      "Epoch 297/1024\n",
      "9/9 - 1s - loss: 0.4115\n",
      "Epoch 298/1024\n",
      "9/9 - 1s - loss: 0.4123\n",
      "Epoch 299/1024\n",
      "9/9 - 1s - loss: 0.4135\n",
      "Epoch 300/1024\n",
      "9/9 - 1s - loss: 0.4117\n",
      "Epoch 301/1024\n",
      "9/9 - 1s - loss: 0.4100\n",
      "Epoch 302/1024\n",
      "9/9 - 1s - loss: 0.4105\n",
      "Epoch 303/1024\n",
      "9/9 - 1s - loss: 0.4135\n",
      "Epoch 304/1024\n",
      "9/9 - 1s - loss: 0.4155\n",
      "Epoch 305/1024\n",
      "9/9 - 1s - loss: 0.4085\n",
      "Epoch 306/1024\n",
      "9/9 - 1s - loss: 0.4101\n",
      "Epoch 307/1024\n",
      "9/9 - 1s - loss: 0.4080\n",
      "Epoch 308/1024\n",
      "9/9 - 1s - loss: 0.4106\n",
      "Epoch 309/1024\n",
      "9/9 - 1s - loss: 0.4087\n",
      "Epoch 310/1024\n",
      "9/9 - 1s - loss: 0.4073\n",
      "Epoch 311/1024\n",
      "9/9 - 1s - loss: 0.4101\n",
      "Epoch 312/1024\n",
      "9/9 - 1s - loss: 0.4093\n",
      "Epoch 313/1024\n",
      "9/9 - 1s - loss: 0.4042\n",
      "Epoch 314/1024\n",
      "9/9 - 1s - loss: 0.4118\n",
      "Epoch 315/1024\n",
      "9/9 - 1s - loss: 0.4113\n",
      "Epoch 316/1024\n",
      "9/9 - 1s - loss: 0.4067\n",
      "Epoch 317/1024\n",
      "9/9 - 1s - loss: 0.4068\n",
      "Epoch 318/1024\n",
      "9/9 - 1s - loss: 0.4126\n",
      "Epoch 319/1024\n",
      "9/9 - 1s - loss: 0.4087\n",
      "Epoch 320/1024\n",
      "9/9 - 1s - loss: 0.4087\n",
      "Epoch 321/1024\n",
      "9/9 - 1s - loss: 0.4096\n",
      "Epoch 322/1024\n",
      "9/9 - 1s - loss: 0.4046\n",
      "Epoch 323/1024\n",
      "9/9 - 1s - loss: 0.4098\n",
      "Epoch 324/1024\n",
      "9/9 - 1s - loss: 0.4070\n",
      "Epoch 325/1024\n",
      "9/9 - 1s - loss: 0.4063\n",
      "Epoch 326/1024\n",
      "9/9 - 1s - loss: 0.4037\n",
      "Epoch 327/1024\n",
      "9/9 - 1s - loss: 0.4069\n",
      "Epoch 328/1024\n",
      "9/9 - 1s - loss: 0.4025\n",
      "Epoch 329/1024\n",
      "9/9 - 1s - loss: 0.4070\n",
      "Epoch 330/1024\n",
      "9/9 - 1s - loss: 0.4021\n",
      "Epoch 331/1024\n",
      "9/9 - 1s - loss: 0.4035\n",
      "Epoch 332/1024\n",
      "9/9 - 1s - loss: 0.4045\n",
      "Epoch 333/1024\n",
      "9/9 - 1s - loss: 0.4009\n",
      "Epoch 334/1024\n",
      "9/9 - 1s - loss: 0.4022\n",
      "Epoch 335/1024\n",
      "9/9 - 1s - loss: 0.4033\n",
      "Epoch 336/1024\n",
      "9/9 - 1s - loss: 0.4018\n",
      "Epoch 337/1024\n",
      "9/9 - 1s - loss: 0.4036\n",
      "Epoch 338/1024\n",
      "9/9 - 1s - loss: 0.4039\n",
      "Epoch 339/1024\n",
      "9/9 - 1s - loss: 0.4027\n",
      "Epoch 340/1024\n",
      "9/9 - 1s - loss: 0.4013\n",
      "Epoch 341/1024\n",
      "9/9 - 1s - loss: 0.4044\n",
      "Epoch 342/1024\n",
      "9/9 - 1s - loss: 0.4015\n",
      "Epoch 343/1024\n",
      "9/9 - 1s - loss: 0.4039\n",
      "Epoch 344/1024\n",
      "9/9 - 1s - loss: 0.4029\n",
      "Epoch 345/1024\n",
      "9/9 - 1s - loss: 0.4018\n",
      "Epoch 346/1024\n",
      "9/9 - 1s - loss: 0.3969\n",
      "Epoch 347/1024\n",
      "9/9 - 1s - loss: 0.4052\n",
      "Epoch 348/1024\n",
      "9/9 - 1s - loss: 0.3999\n",
      "Epoch 349/1024\n",
      "9/9 - 1s - loss: 0.4027\n",
      "Epoch 350/1024\n",
      "9/9 - 1s - loss: 0.4013\n",
      "Epoch 351/1024\n",
      "9/9 - 1s - loss: 0.3995\n",
      "Epoch 352/1024\n",
      "9/9 - 1s - loss: 0.3994\n",
      "Epoch 353/1024\n",
      "9/9 - 1s - loss: 0.3991\n",
      "Epoch 354/1024\n",
      "9/9 - 1s - loss: 0.3985\n",
      "Epoch 355/1024\n",
      "9/9 - 1s - loss: 0.4013\n",
      "Epoch 356/1024\n",
      "9/9 - 1s - loss: 0.4032\n",
      "Epoch 357/1024\n",
      "9/9 - 1s - loss: 0.3996\n",
      "Epoch 358/1024\n",
      "9/9 - 1s - loss: 0.3963\n",
      "Epoch 359/1024\n",
      "9/9 - 1s - loss: 0.3974\n",
      "Epoch 360/1024\n",
      "9/9 - 1s - loss: 0.4011\n",
      "Epoch 361/1024\n",
      "9/9 - 1s - loss: 0.3992\n",
      "Epoch 362/1024\n",
      "9/9 - 1s - loss: 0.3990\n",
      "Epoch 363/1024\n",
      "9/9 - 1s - loss: 0.3970\n",
      "Epoch 364/1024\n",
      "9/9 - 1s - loss: 0.3971\n",
      "Epoch 365/1024\n",
      "9/9 - 1s - loss: 0.3968\n",
      "Epoch 366/1024\n",
      "9/9 - 1s - loss: 0.3988\n",
      "Epoch 367/1024\n",
      "9/9 - 1s - loss: 0.3983\n",
      "Epoch 368/1024\n",
      "9/9 - 1s - loss: 0.3948\n",
      "Epoch 369/1024\n",
      "9/9 - 1s - loss: 0.3995\n",
      "Epoch 370/1024\n",
      "9/9 - 1s - loss: 0.3967\n",
      "Epoch 371/1024\n",
      "9/9 - 1s - loss: 0.3943\n",
      "Epoch 372/1024\n",
      "9/9 - 1s - loss: 0.3985\n",
      "Epoch 373/1024\n",
      "9/9 - 1s - loss: 0.3945\n",
      "Epoch 374/1024\n",
      "9/9 - 1s - loss: 0.3953\n",
      "Epoch 375/1024\n",
      "9/9 - 1s - loss: 0.3970\n",
      "Epoch 376/1024\n",
      "9/9 - 1s - loss: 0.3996\n",
      "Epoch 377/1024\n",
      "9/9 - 1s - loss: 0.3953\n",
      "Epoch 378/1024\n",
      "9/9 - 1s - loss: 0.3948\n",
      "Epoch 379/1024\n",
      "9/9 - 1s - loss: 0.3934\n",
      "Epoch 380/1024\n",
      "9/9 - 1s - loss: 0.3951\n",
      "Epoch 381/1024\n",
      "9/9 - 1s - loss: 0.3948\n",
      "Epoch 382/1024\n",
      "9/9 - 1s - loss: 0.3957\n",
      "Epoch 383/1024\n",
      "9/9 - 1s - loss: 0.3961\n",
      "Epoch 384/1024\n",
      "9/9 - 1s - loss: 0.3921\n",
      "Epoch 385/1024\n",
      "9/9 - 1s - loss: 0.3933\n",
      "Epoch 386/1024\n",
      "9/9 - 1s - loss: 0.3935\n",
      "Epoch 387/1024\n",
      "9/9 - 1s - loss: 0.3923\n",
      "Epoch 388/1024\n",
      "9/9 - 1s - loss: 0.3933\n",
      "Epoch 389/1024\n",
      "9/9 - 1s - loss: 0.3900\n",
      "Epoch 390/1024\n",
      "9/9 - 1s - loss: 0.3893\n",
      "Epoch 391/1024\n",
      "9/9 - 1s - loss: 0.3897\n",
      "Epoch 392/1024\n",
      "9/9 - 1s - loss: 0.3905\n",
      "Epoch 393/1024\n",
      "9/9 - 1s - loss: 0.3889\n",
      "Epoch 394/1024\n",
      "9/9 - 1s - loss: 0.3931\n",
      "Epoch 395/1024\n",
      "9/9 - 1s - loss: 0.3931\n",
      "Epoch 396/1024\n",
      "9/9 - 1s - loss: 0.3912\n",
      "Epoch 397/1024\n",
      "9/9 - 1s - loss: 0.3874\n",
      "Epoch 398/1024\n",
      "9/9 - 1s - loss: 0.3892\n",
      "Epoch 399/1024\n",
      "9/9 - 1s - loss: 0.3907\n",
      "Epoch 400/1024\n",
      "9/9 - 1s - loss: 0.3870\n",
      "Epoch 401/1024\n",
      "9/9 - 1s - loss: 0.3926\n",
      "Epoch 402/1024\n",
      "9/9 - 1s - loss: 0.3923\n",
      "Epoch 403/1024\n",
      "9/9 - 1s - loss: 0.3861\n",
      "Epoch 404/1024\n",
      "9/9 - 1s - loss: 0.3875\n",
      "Epoch 405/1024\n",
      "9/9 - 1s - loss: 0.3910\n",
      "Epoch 406/1024\n",
      "9/9 - 1s - loss: 0.3879\n",
      "Epoch 407/1024\n",
      "9/9 - 1s - loss: 0.3891\n",
      "Epoch 408/1024\n",
      "9/9 - 1s - loss: 0.3880\n",
      "Epoch 409/1024\n",
      "9/9 - 1s - loss: 0.3909\n",
      "Epoch 410/1024\n",
      "9/9 - 1s - loss: 0.3883\n",
      "Epoch 411/1024\n",
      "9/9 - 1s - loss: 0.3859\n",
      "Epoch 412/1024\n",
      "9/9 - 1s - loss: 0.3873\n",
      "Epoch 413/1024\n",
      "9/9 - 1s - loss: 0.3882\n",
      "Epoch 414/1024\n",
      "9/9 - 1s - loss: 0.3860\n",
      "Epoch 415/1024\n",
      "9/9 - 1s - loss: 0.3920\n",
      "Epoch 416/1024\n",
      "9/9 - 1s - loss: 0.3863\n",
      "Epoch 417/1024\n",
      "9/9 - 1s - loss: 0.3888\n",
      "Epoch 418/1024\n",
      "9/9 - 1s - loss: 0.3860\n",
      "Epoch 419/1024\n",
      "9/9 - 1s - loss: 0.3855\n",
      "Epoch 420/1024\n",
      "9/9 - 1s - loss: 0.3839\n",
      "Epoch 421/1024\n",
      "9/9 - 1s - loss: 0.3832\n",
      "Epoch 422/1024\n",
      "9/9 - 1s - loss: 0.3868\n",
      "Epoch 423/1024\n",
      "9/9 - 1s - loss: 0.3843\n",
      "Epoch 424/1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 1s - loss: 0.3848\n",
      "Epoch 425/1024\n",
      "9/9 - 1s - loss: 0.3838\n",
      "Epoch 426/1024\n",
      "9/9 - 1s - loss: 0.3862\n",
      "Epoch 427/1024\n",
      "9/9 - 1s - loss: 0.3857\n",
      "Epoch 428/1024\n",
      "9/9 - 1s - loss: 0.3829\n",
      "Epoch 429/1024\n",
      "9/9 - 1s - loss: 0.3814\n",
      "Epoch 430/1024\n",
      "9/9 - 1s - loss: 0.3860\n",
      "Epoch 431/1024\n",
      "9/9 - 1s - loss: 0.3860\n",
      "Epoch 432/1024\n",
      "9/9 - 1s - loss: 0.3831\n",
      "Epoch 433/1024\n",
      "9/9 - 1s - loss: 0.3803\n",
      "Epoch 434/1024\n",
      "9/9 - 1s - loss: 0.3796\n",
      "Epoch 435/1024\n",
      "9/9 - 1s - loss: 0.3820\n",
      "Epoch 436/1024\n",
      "9/9 - 1s - loss: 0.3821\n",
      "Epoch 437/1024\n",
      "9/9 - 1s - loss: 0.3838\n",
      "Epoch 438/1024\n",
      "9/9 - 1s - loss: 0.3855\n",
      "Epoch 439/1024\n",
      "9/9 - 1s - loss: 0.3828\n",
      "Epoch 440/1024\n",
      "9/9 - 1s - loss: 0.3801\n",
      "Epoch 441/1024\n",
      "9/9 - 1s - loss: 0.3824\n",
      "Epoch 442/1024\n",
      "9/9 - 1s - loss: 0.3802\n",
      "Epoch 443/1024\n",
      "9/9 - 1s - loss: 0.3789\n",
      "Epoch 444/1024\n",
      "9/9 - 1s - loss: 0.3839\n",
      "Epoch 445/1024\n",
      "9/9 - 1s - loss: 0.3803\n",
      "Epoch 446/1024\n",
      "9/9 - 1s - loss: 0.3824\n",
      "Epoch 447/1024\n",
      "9/9 - 1s - loss: 0.3772\n",
      "Epoch 448/1024\n",
      "9/9 - 1s - loss: 0.3834\n",
      "Epoch 449/1024\n",
      "9/9 - 1s - loss: 0.3816\n",
      "Epoch 450/1024\n",
      "9/9 - 1s - loss: 0.3789\n",
      "Epoch 451/1024\n",
      "9/9 - 1s - loss: 0.3847\n",
      "Epoch 452/1024\n",
      "9/9 - 1s - loss: 0.3824\n",
      "Epoch 453/1024\n",
      "9/9 - 1s - loss: 0.3792\n",
      "Epoch 454/1024\n",
      "9/9 - 1s - loss: 0.3779\n",
      "Epoch 455/1024\n",
      "9/9 - 1s - loss: 0.3808\n",
      "Epoch 456/1024\n",
      "9/9 - 1s - loss: 0.3831\n",
      "Epoch 457/1024\n",
      "9/9 - 1s - loss: 0.3799\n",
      "Epoch 458/1024\n",
      "9/9 - 1s - loss: 0.3813\n",
      "Epoch 459/1024\n",
      "9/9 - 1s - loss: 0.3788\n",
      "Epoch 460/1024\n",
      "9/9 - 1s - loss: 0.3750\n",
      "Epoch 461/1024\n",
      "9/9 - 1s - loss: 0.3785\n",
      "Epoch 462/1024\n",
      "9/9 - 1s - loss: 0.3775\n",
      "Epoch 463/1024\n",
      "9/9 - 1s - loss: 0.3805\n",
      "Epoch 464/1024\n",
      "9/9 - 1s - loss: 0.3813\n",
      "Epoch 465/1024\n",
      "9/9 - 1s - loss: 0.3783\n",
      "Epoch 466/1024\n",
      "9/9 - 1s - loss: 0.3818\n",
      "Epoch 467/1024\n",
      "9/9 - 1s - loss: 0.3776\n",
      "Epoch 468/1024\n",
      "9/9 - 1s - loss: 0.3760\n",
      "Epoch 469/1024\n",
      "9/9 - 1s - loss: 0.3770\n",
      "Epoch 470/1024\n",
      "9/9 - 1s - loss: 0.3793\n",
      "Epoch 471/1024\n",
      "9/9 - 1s - loss: 0.3772\n",
      "Epoch 472/1024\n",
      "9/9 - 1s - loss: 0.3789\n",
      "Epoch 473/1024\n",
      "9/9 - 1s - loss: 0.3785\n",
      "Epoch 474/1024\n",
      "9/9 - 1s - loss: 0.3785\n",
      "Epoch 475/1024\n",
      "9/9 - 1s - loss: 0.3722\n",
      "Epoch 476/1024\n",
      "9/9 - 1s - loss: 0.3776\n",
      "Epoch 477/1024\n",
      "9/9 - 1s - loss: 0.3758\n",
      "Epoch 478/1024\n",
      "9/9 - 1s - loss: 0.3766\n",
      "Epoch 479/1024\n",
      "9/9 - 1s - loss: 0.3750\n",
      "Epoch 480/1024\n",
      "9/9 - 1s - loss: 0.3735\n",
      "Epoch 481/1024\n",
      "9/9 - 1s - loss: 0.3786\n",
      "Epoch 482/1024\n",
      "9/9 - 1s - loss: 0.3772\n",
      "Epoch 483/1024\n",
      "9/9 - 1s - loss: 0.3747\n",
      "Epoch 484/1024\n",
      "9/9 - 1s - loss: 0.3763\n",
      "Epoch 485/1024\n",
      "9/9 - 1s - loss: 0.3748\n",
      "Epoch 486/1024\n",
      "9/9 - 1s - loss: 0.3742\n",
      "Epoch 487/1024\n",
      "9/9 - 1s - loss: 0.3748\n",
      "Epoch 488/1024\n",
      "9/9 - 1s - loss: 0.3754\n",
      "Epoch 489/1024\n",
      "9/9 - 1s - loss: 0.3763\n",
      "Epoch 490/1024\n",
      "9/9 - 1s - loss: 0.3770\n",
      "Epoch 491/1024\n",
      "9/9 - 1s - loss: 0.3745\n",
      "Epoch 492/1024\n",
      "9/9 - 1s - loss: 0.3719\n",
      "Epoch 493/1024\n",
      "9/9 - 1s - loss: 0.3739\n",
      "Epoch 494/1024\n",
      "9/9 - 1s - loss: 0.3702\n",
      "Epoch 495/1024\n",
      "9/9 - 1s - loss: 0.3715\n",
      "Epoch 496/1024\n",
      "9/9 - 1s - loss: 0.3770\n",
      "Epoch 497/1024\n",
      "9/9 - 1s - loss: 0.3703\n",
      "Epoch 498/1024\n",
      "9/9 - 1s - loss: 0.3741\n",
      "Epoch 499/1024\n",
      "9/9 - 1s - loss: 0.3752\n",
      "Epoch 500/1024\n",
      "9/9 - 1s - loss: 0.3724\n",
      "Epoch 501/1024\n",
      "9/9 - 1s - loss: 0.3726\n",
      "Epoch 502/1024\n",
      "9/9 - 1s - loss: 0.3737\n",
      "Epoch 503/1024\n",
      "9/9 - 1s - loss: 0.3741\n",
      "Epoch 504/1024\n",
      "9/9 - 1s - loss: 0.3732\n",
      "Epoch 505/1024\n",
      "9/9 - 1s - loss: 0.3717\n",
      "Epoch 506/1024\n",
      "9/9 - 1s - loss: 0.3703\n",
      "Epoch 507/1024\n",
      "9/9 - 1s - loss: 0.3733\n",
      "Epoch 508/1024\n",
      "9/9 - 1s - loss: 0.3703\n",
      "Epoch 509/1024\n",
      "9/9 - 1s - loss: 0.3712\n",
      "Epoch 510/1024\n",
      "9/9 - 1s - loss: 0.3713\n",
      "Epoch 511/1024\n",
      "9/9 - 1s - loss: 0.3713\n",
      "Epoch 512/1024\n",
      "9/9 - 1s - loss: 0.3680\n",
      "Epoch 513/1024\n",
      "9/9 - 1s - loss: 0.3743\n",
      "Epoch 514/1024\n",
      "9/9 - 1s - loss: 0.3705\n",
      "Epoch 515/1024\n",
      "9/9 - 1s - loss: 0.3731\n",
      "Epoch 516/1024\n",
      "9/9 - 1s - loss: 0.3704\n",
      "Epoch 517/1024\n",
      "9/9 - 1s - loss: 0.3706\n",
      "Epoch 518/1024\n",
      "9/9 - 1s - loss: 0.3678\n",
      "Epoch 519/1024\n",
      "9/9 - 1s - loss: 0.3733\n",
      "Epoch 520/1024\n",
      "9/9 - 1s - loss: 0.3709\n",
      "Epoch 521/1024\n",
      "9/9 - 1s - loss: 0.3733\n",
      "Epoch 522/1024\n",
      "9/9 - 1s - loss: 0.3688\n",
      "Epoch 523/1024\n",
      "9/9 - 1s - loss: 0.3702\n",
      "Epoch 524/1024\n",
      "9/9 - 1s - loss: 0.3734\n",
      "Epoch 525/1024\n",
      "9/9 - 1s - loss: 0.3684\n",
      "Epoch 526/1024\n",
      "9/9 - 1s - loss: 0.3701\n",
      "Epoch 527/1024\n",
      "9/9 - 1s - loss: 0.3690\n",
      "Epoch 528/1024\n",
      "9/9 - 1s - loss: 0.3675\n",
      "Epoch 529/1024\n",
      "9/9 - 1s - loss: 0.3667\n",
      "Epoch 530/1024\n",
      "9/9 - 1s - loss: 0.3666\n",
      "Epoch 531/1024\n",
      "9/9 - 1s - loss: 0.3698\n",
      "Epoch 532/1024\n",
      "9/9 - 1s - loss: 0.3695\n",
      "Epoch 533/1024\n",
      "9/9 - 1s - loss: 0.3701\n",
      "Epoch 534/1024\n",
      "9/9 - 1s - loss: 0.3673\n",
      "Epoch 535/1024\n",
      "9/9 - 1s - loss: 0.3682\n",
      "Epoch 536/1024\n",
      "9/9 - 1s - loss: 0.3672\n",
      "Epoch 537/1024\n",
      "9/9 - 1s - loss: 0.3713\n",
      "Epoch 538/1024\n",
      "9/9 - 1s - loss: 0.3709\n",
      "Epoch 539/1024\n",
      "9/9 - 1s - loss: 0.3673\n",
      "Epoch 540/1024\n",
      "9/9 - 1s - loss: 0.3645\n",
      "Epoch 541/1024\n",
      "9/9 - 1s - loss: 0.3716\n",
      "Epoch 542/1024\n",
      "9/9 - 1s - loss: 0.3676\n",
      "Epoch 543/1024\n",
      "9/9 - 1s - loss: 0.3668\n",
      "Epoch 544/1024\n",
      "9/9 - 1s - loss: 0.3658\n",
      "Epoch 545/1024\n",
      "9/9 - 1s - loss: 0.3662\n",
      "Epoch 546/1024\n",
      "9/9 - 1s - loss: 0.3640\n",
      "Epoch 547/1024\n",
      "9/9 - 1s - loss: 0.3703\n",
      "Epoch 548/1024\n",
      "9/9 - 1s - loss: 0.3718\n",
      "Epoch 549/1024\n",
      "9/9 - 1s - loss: 0.3666\n",
      "Epoch 550/1024\n",
      "9/9 - 1s - loss: 0.3665\n",
      "Epoch 551/1024\n",
      "9/9 - 1s - loss: 0.3668\n",
      "Epoch 552/1024\n",
      "9/9 - 1s - loss: 0.3643\n",
      "Epoch 553/1024\n",
      "9/9 - 1s - loss: 0.3644\n",
      "Epoch 554/1024\n",
      "9/9 - 1s - loss: 0.3654\n",
      "Epoch 555/1024\n",
      "9/9 - 1s - loss: 0.3661\n",
      "Epoch 556/1024\n",
      "9/9 - 1s - loss: 0.3682\n",
      "Epoch 557/1024\n",
      "9/9 - 1s - loss: 0.3633\n",
      "Epoch 558/1024\n",
      "9/9 - 1s - loss: 0.3632\n",
      "Epoch 559/1024\n",
      "9/9 - 1s - loss: 0.3637\n",
      "Epoch 560/1024\n",
      "9/9 - 1s - loss: 0.3631\n",
      "Epoch 561/1024\n",
      "9/9 - 1s - loss: 0.3642\n",
      "Epoch 562/1024\n",
      "9/9 - 1s - loss: 0.3650\n",
      "Epoch 563/1024\n",
      "9/9 - 1s - loss: 0.3642\n",
      "Epoch 564/1024\n",
      "9/9 - 1s - loss: 0.3617\n",
      "Epoch 565/1024\n",
      "9/9 - 1s - loss: 0.3645\n",
      "Epoch 566/1024\n",
      "9/9 - 1s - loss: 0.3662\n",
      "Epoch 567/1024\n",
      "9/9 - 1s - loss: 0.3685\n",
      "Epoch 568/1024\n",
      "9/9 - 1s - loss: 0.3614\n",
      "Epoch 569/1024\n",
      "9/9 - 1s - loss: 0.3643\n",
      "Epoch 570/1024\n",
      "9/9 - 1s - loss: 0.3650\n",
      "Epoch 571/1024\n",
      "9/9 - 1s - loss: 0.3614\n",
      "Epoch 572/1024\n",
      "9/9 - 1s - loss: 0.3619\n",
      "Epoch 573/1024\n",
      "9/9 - 1s - loss: 0.3622\n",
      "Epoch 574/1024\n",
      "9/9 - 1s - loss: 0.3651\n",
      "Epoch 575/1024\n",
      "9/9 - 1s - loss: 0.3637\n",
      "Epoch 576/1024\n",
      "9/9 - 1s - loss: 0.3609\n",
      "Epoch 577/1024\n",
      "9/9 - 1s - loss: 0.3627\n",
      "Epoch 578/1024\n",
      "9/9 - 1s - loss: 0.3616\n",
      "Epoch 579/1024\n",
      "9/9 - 1s - loss: 0.3615\n",
      "Epoch 580/1024\n",
      "9/9 - 1s - loss: 0.3648\n",
      "Epoch 581/1024\n",
      "9/9 - 1s - loss: 0.3598\n",
      "Epoch 582/1024\n",
      "9/9 - 1s - loss: 0.3618\n",
      "Epoch 583/1024\n",
      "9/9 - 1s - loss: 0.3612\n",
      "Epoch 584/1024\n",
      "9/9 - 1s - loss: 0.3587\n",
      "Epoch 585/1024\n",
      "9/9 - 1s - loss: 0.3629\n",
      "Epoch 586/1024\n",
      "9/9 - 1s - loss: 0.3596\n",
      "Epoch 587/1024\n",
      "9/9 - 1s - loss: 0.3620\n",
      "Epoch 588/1024\n",
      "9/9 - 1s - loss: 0.3630\n",
      "Epoch 589/1024\n",
      "9/9 - 1s - loss: 0.3593\n",
      "Epoch 590/1024\n",
      "9/9 - 1s - loss: 0.3634\n",
      "Epoch 591/1024\n",
      "9/9 - 1s - loss: 0.3584\n",
      "Epoch 592/1024\n",
      "9/9 - 1s - loss: 0.3573\n",
      "Epoch 593/1024\n",
      "9/9 - 1s - loss: 0.3596\n",
      "Epoch 594/1024\n",
      "9/9 - 1s - loss: 0.3644\n",
      "Epoch 595/1024\n",
      "9/9 - 1s - loss: 0.3613\n",
      "Epoch 596/1024\n",
      "9/9 - 1s - loss: 0.3594\n",
      "Epoch 597/1024\n",
      "9/9 - 1s - loss: 0.3598\n",
      "Epoch 598/1024\n",
      "9/9 - 1s - loss: 0.3552\n",
      "Epoch 599/1024\n",
      "9/9 - 1s - loss: 0.3611\n",
      "Epoch 600/1024\n",
      "9/9 - 1s - loss: 0.3596\n",
      "Epoch 601/1024\n",
      "9/9 - 1s - loss: 0.3629\n",
      "Epoch 602/1024\n",
      "9/9 - 1s - loss: 0.3586\n",
      "Epoch 603/1024\n",
      "9/9 - 1s - loss: 0.3581\n",
      "Epoch 604/1024\n",
      "9/9 - 1s - loss: 0.3572\n",
      "Epoch 605/1024\n",
      "9/9 - 1s - loss: 0.3589\n",
      "Epoch 606/1024\n",
      "9/9 - 1s - loss: 0.3568\n",
      "Epoch 607/1024\n",
      "9/9 - 1s - loss: 0.3619\n",
      "Epoch 608/1024\n",
      "9/9 - 1s - loss: 0.3607\n",
      "Epoch 609/1024\n",
      "9/9 - 1s - loss: 0.3636\n",
      "Epoch 610/1024\n",
      "9/9 - 1s - loss: 0.3610\n",
      "Epoch 611/1024\n",
      "9/9 - 1s - loss: 0.3595\n",
      "Epoch 612/1024\n",
      "9/9 - 1s - loss: 0.3631\n",
      "Epoch 613/1024\n",
      "9/9 - 1s - loss: 0.3558\n",
      "Epoch 614/1024\n",
      "9/9 - 1s - loss: 0.3594\n",
      "Epoch 615/1024\n",
      "9/9 - 1s - loss: 0.3594\n",
      "Epoch 616/1024\n",
      "9/9 - 1s - loss: 0.3597\n",
      "Epoch 617/1024\n",
      "9/9 - 1s - loss: 0.3552\n",
      "Epoch 618/1024\n",
      "9/9 - 1s - loss: 0.3606\n",
      "Epoch 619/1024\n",
      "9/9 - 1s - loss: 0.3598\n",
      "Epoch 620/1024\n",
      "9/9 - 1s - loss: 0.3569\n",
      "Epoch 621/1024\n",
      "9/9 - 1s - loss: 0.3583\n",
      "Epoch 622/1024\n",
      "9/9 - 1s - loss: 0.3611\n",
      "Epoch 623/1024\n",
      "9/9 - 1s - loss: 0.3584\n",
      "Epoch 624/1024\n",
      "9/9 - 1s - loss: 0.3590\n",
      "Epoch 625/1024\n",
      "9/9 - 1s - loss: 0.3611\n",
      "Epoch 626/1024\n",
      "9/9 - 1s - loss: 0.3590\n",
      "Epoch 627/1024\n",
      "9/9 - 1s - loss: 0.3604\n",
      "Epoch 628/1024\n",
      "9/9 - 1s - loss: 0.3558\n",
      "Epoch 629/1024\n",
      "9/9 - 1s - loss: 0.3588\n",
      "Epoch 630/1024\n",
      "9/9 - 1s - loss: 0.3577\n",
      "Epoch 631/1024\n",
      "9/9 - 1s - loss: 0.3538\n",
      "Epoch 632/1024\n",
      "9/9 - 1s - loss: 0.3556\n",
      "Epoch 633/1024\n",
      "9/9 - 1s - loss: 0.3571\n",
      "Epoch 634/1024\n",
      "9/9 - 1s - loss: 0.3593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 635/1024\n",
      "9/9 - 1s - loss: 0.3562\n",
      "Epoch 636/1024\n",
      "9/9 - 1s - loss: 0.3574\n",
      "Epoch 637/1024\n",
      "9/9 - 1s - loss: 0.3559\n",
      "Epoch 638/1024\n",
      "9/9 - 1s - loss: 0.3590\n",
      "Epoch 639/1024\n",
      "9/9 - 1s - loss: 0.3575\n",
      "Epoch 640/1024\n",
      "9/9 - 1s - loss: 0.3549\n",
      "Epoch 641/1024\n",
      "9/9 - 1s - loss: 0.3511\n",
      "Epoch 642/1024\n",
      "9/9 - 1s - loss: 0.3581\n",
      "Epoch 643/1024\n",
      "9/9 - 1s - loss: 0.3556\n",
      "Epoch 644/1024\n",
      "9/9 - 1s - loss: 0.3590\n",
      "Epoch 645/1024\n",
      "9/9 - 1s - loss: 0.3592\n",
      "Epoch 646/1024\n",
      "9/9 - 1s - loss: 0.3584\n",
      "Epoch 647/1024\n",
      "9/9 - 1s - loss: 0.3554\n",
      "Epoch 648/1024\n",
      "9/9 - 1s - loss: 0.3565\n",
      "Epoch 649/1024\n",
      "9/9 - 1s - loss: 0.3574\n",
      "Epoch 650/1024\n",
      "9/9 - 1s - loss: 0.3563\n",
      "Epoch 651/1024\n",
      "9/9 - 1s - loss: 0.3556\n",
      "Epoch 652/1024\n",
      "9/9 - 1s - loss: 0.3556\n",
      "Epoch 653/1024\n",
      "9/9 - 1s - loss: 0.3543\n",
      "Epoch 654/1024\n",
      "9/9 - 1s - loss: 0.3520\n",
      "Epoch 655/1024\n",
      "9/9 - 1s - loss: 0.3530\n",
      "Epoch 656/1024\n",
      "9/9 - 1s - loss: 0.3559\n",
      "Epoch 657/1024\n",
      "9/9 - 1s - loss: 0.3551\n",
      "Epoch 658/1024\n",
      "9/9 - 1s - loss: 0.3571\n",
      "Epoch 659/1024\n",
      "9/9 - 1s - loss: 0.3569\n",
      "Epoch 660/1024\n",
      "9/9 - 1s - loss: 0.3575\n",
      "Epoch 661/1024\n",
      "9/9 - 1s - loss: 0.3556\n",
      "Epoch 662/1024\n",
      "9/9 - 1s - loss: 0.3529\n",
      "Epoch 663/1024\n",
      "9/9 - 1s - loss: 0.3583\n",
      "Epoch 664/1024\n",
      "9/9 - 1s - loss: 0.3536\n",
      "Epoch 665/1024\n",
      "9/9 - 1s - loss: 0.3533\n",
      "Epoch 666/1024\n",
      "9/9 - 1s - loss: 0.3590\n",
      "Epoch 667/1024\n",
      "9/9 - 1s - loss: 0.3553\n",
      "Epoch 668/1024\n",
      "9/9 - 1s - loss: 0.3543\n",
      "Epoch 669/1024\n",
      "9/9 - 1s - loss: 0.3574\n",
      "Epoch 670/1024\n",
      "9/9 - 1s - loss: 0.3565\n",
      "Epoch 671/1024\n",
      "9/9 - 1s - loss: 0.3528\n",
      "Epoch 672/1024\n",
      "9/9 - 1s - loss: 0.3547\n",
      "Epoch 673/1024\n",
      "9/9 - 1s - loss: 0.3543\n",
      "Epoch 674/1024\n",
      "9/9 - 1s - loss: 0.3538\n",
      "Epoch 675/1024\n",
      "9/9 - 1s - loss: 0.3510\n",
      "Epoch 676/1024\n",
      "9/9 - 1s - loss: 0.3559\n",
      "Epoch 677/1024\n",
      "9/9 - 1s - loss: 0.3556\n",
      "Epoch 678/1024\n",
      "9/9 - 1s - loss: 0.3547\n",
      "Epoch 679/1024\n",
      "9/9 - 1s - loss: 0.3518\n",
      "Epoch 680/1024\n",
      "9/9 - 1s - loss: 0.3561\n",
      "Epoch 681/1024\n",
      "9/9 - 1s - loss: 0.3518\n",
      "Epoch 682/1024\n",
      "9/9 - 1s - loss: 0.3531\n",
      "Epoch 683/1024\n",
      "9/9 - 1s - loss: 0.3569\n",
      "Epoch 684/1024\n",
      "9/9 - 1s - loss: 0.3527\n",
      "Epoch 685/1024\n",
      "9/9 - 1s - loss: 0.3537\n",
      "Epoch 686/1024\n",
      "9/9 - 1s - loss: 0.3557\n",
      "Epoch 687/1024\n",
      "9/9 - 1s - loss: 0.3557\n",
      "Epoch 688/1024\n",
      "9/9 - 1s - loss: 0.3520\n",
      "Epoch 689/1024\n",
      "9/9 - 1s - loss: 0.3572\n",
      "Epoch 690/1024\n",
      "9/9 - 1s - loss: 0.3489\n",
      "Epoch 691/1024\n",
      "9/9 - 1s - loss: 0.3522\n",
      "Epoch 692/1024\n",
      "9/9 - 1s - loss: 0.3541\n",
      "Epoch 693/1024\n",
      "9/9 - 1s - loss: 0.3535\n",
      "Epoch 694/1024\n",
      "9/9 - 1s - loss: 0.3573\n",
      "Epoch 695/1024\n",
      "9/9 - 1s - loss: 0.3559\n",
      "Epoch 696/1024\n",
      "9/9 - 1s - loss: 0.3532\n",
      "Epoch 697/1024\n",
      "9/9 - 1s - loss: 0.3528\n",
      "Epoch 698/1024\n",
      "9/9 - 1s - loss: 0.3534\n",
      "Epoch 699/1024\n",
      "9/9 - 1s - loss: 0.3515\n",
      "Epoch 700/1024\n",
      "9/9 - 1s - loss: 0.3520\n",
      "Epoch 701/1024\n",
      "9/9 - 1s - loss: 0.3503\n",
      "Epoch 702/1024\n",
      "9/9 - 1s - loss: 0.3511\n",
      "Epoch 703/1024\n",
      "9/9 - 1s - loss: 0.3470\n",
      "Epoch 704/1024\n",
      "9/9 - 1s - loss: 0.3538\n",
      "Epoch 705/1024\n",
      "9/9 - 1s - loss: 0.3515\n",
      "Epoch 706/1024\n",
      "9/9 - 1s - loss: 0.3526\n",
      "Epoch 707/1024\n",
      "9/9 - 1s - loss: 0.3515\n",
      "Epoch 708/1024\n",
      "9/9 - 1s - loss: 0.3497\n",
      "Epoch 709/1024\n",
      "9/9 - 1s - loss: 0.3529\n",
      "Epoch 710/1024\n",
      "9/9 - 1s - loss: 0.3526\n",
      "Epoch 711/1024\n",
      "9/9 - 1s - loss: 0.3507\n",
      "Epoch 712/1024\n",
      "9/9 - 1s - loss: 0.3492\n",
      "Epoch 713/1024\n",
      "9/9 - 1s - loss: 0.3539\n",
      "Epoch 714/1024\n",
      "9/9 - 1s - loss: 0.3494\n",
      "Epoch 715/1024\n",
      "9/9 - 1s - loss: 0.3559\n",
      "Epoch 716/1024\n",
      "9/9 - 1s - loss: 0.3493\n",
      "Epoch 717/1024\n",
      "9/9 - 1s - loss: 0.3488\n",
      "Epoch 718/1024\n",
      "9/9 - 1s - loss: 0.3508\n",
      "Epoch 719/1024\n",
      "9/9 - 1s - loss: 0.3511\n",
      "Epoch 720/1024\n",
      "9/9 - 1s - loss: 0.3516\n",
      "Epoch 721/1024\n",
      "9/9 - 1s - loss: 0.3499\n",
      "Epoch 722/1024\n",
      "9/9 - 1s - loss: 0.3529\n",
      "Epoch 723/1024\n",
      "9/9 - 1s - loss: 0.3512\n",
      "Epoch 724/1024\n",
      "9/9 - 1s - loss: 0.3541\n",
      "Epoch 725/1024\n",
      "9/9 - 1s - loss: 0.3512\n",
      "Epoch 726/1024\n",
      "9/9 - 1s - loss: 0.3496\n",
      "Epoch 727/1024\n",
      "9/9 - 1s - loss: 0.3515\n",
      "Epoch 728/1024\n",
      "9/9 - 1s - loss: 0.3536\n",
      "Epoch 729/1024\n",
      "9/9 - 1s - loss: 0.3485\n",
      "Epoch 730/1024\n",
      "9/9 - 1s - loss: 0.3475\n",
      "Epoch 731/1024\n",
      "9/9 - 1s - loss: 0.3513\n",
      "Epoch 732/1024\n",
      "9/9 - 1s - loss: 0.3530\n",
      "Epoch 733/1024\n",
      "9/9 - 1s - loss: 0.3470\n",
      "Epoch 734/1024\n",
      "9/9 - 1s - loss: 0.3507\n",
      "Epoch 735/1024\n",
      "9/9 - 1s - loss: 0.3516\n",
      "Epoch 736/1024\n",
      "9/9 - 1s - loss: 0.3516\n",
      "Epoch 737/1024\n",
      "9/9 - 1s - loss: 0.3501\n",
      "Epoch 738/1024\n",
      "9/9 - 1s - loss: 0.3555\n",
      "Epoch 739/1024\n",
      "9/9 - 1s - loss: 0.3487\n",
      "Epoch 740/1024\n",
      "9/9 - 1s - loss: 0.3508\n",
      "Epoch 741/1024\n",
      "9/9 - 1s - loss: 0.3476\n",
      "Epoch 742/1024\n",
      "9/9 - 1s - loss: 0.3449\n",
      "Epoch 743/1024\n",
      "9/9 - 1s - loss: 0.3486\n",
      "Epoch 744/1024\n",
      "9/9 - 1s - loss: 0.3538\n",
      "Epoch 745/1024\n",
      "9/9 - 1s - loss: 0.3542\n",
      "Epoch 746/1024\n",
      "9/9 - 1s - loss: 0.3524\n",
      "Epoch 747/1024\n",
      "9/9 - 1s - loss: 0.3517\n",
      "Epoch 748/1024\n",
      "9/9 - 1s - loss: 0.3507\n",
      "Epoch 749/1024\n",
      "9/9 - 1s - loss: 0.3491\n",
      "Epoch 750/1024\n",
      "9/9 - 1s - loss: 0.3514\n",
      "Epoch 751/1024\n",
      "9/9 - 1s - loss: 0.3458\n",
      "Epoch 752/1024\n",
      "9/9 - 1s - loss: 0.3497\n",
      "Epoch 753/1024\n",
      "9/9 - 1s - loss: 0.3501\n",
      "Epoch 754/1024\n",
      "9/9 - 1s - loss: 0.3494\n",
      "Epoch 755/1024\n",
      "9/9 - 1s - loss: 0.3484\n",
      "Epoch 756/1024\n",
      "9/9 - 1s - loss: 0.3495\n",
      "Epoch 757/1024\n",
      "9/9 - 1s - loss: 0.3499\n",
      "Epoch 758/1024\n",
      "9/9 - 1s - loss: 0.3503\n",
      "Epoch 759/1024\n",
      "9/9 - 1s - loss: 0.3464\n",
      "Epoch 760/1024\n",
      "9/9 - 1s - loss: 0.3501\n",
      "Epoch 761/1024\n",
      "9/9 - 1s - loss: 0.3471\n",
      "Epoch 762/1024\n",
      "9/9 - 1s - loss: 0.3506\n",
      "Epoch 763/1024\n",
      "9/9 - 1s - loss: 0.3457\n",
      "Epoch 764/1024\n",
      "9/9 - 1s - loss: 0.3484\n",
      "Epoch 765/1024\n",
      "9/9 - 1s - loss: 0.3500\n",
      "Epoch 766/1024\n",
      "9/9 - 1s - loss: 0.3499\n",
      "Epoch 767/1024\n",
      "9/9 - 1s - loss: 0.3525\n",
      "Epoch 768/1024\n",
      "9/9 - 1s - loss: 0.3503\n",
      "Epoch 769/1024\n",
      "9/9 - 1s - loss: 0.3482\n",
      "Epoch 770/1024\n",
      "9/9 - 1s - loss: 0.3455\n",
      "Epoch 771/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 772/1024\n",
      "9/9 - 1s - loss: 0.3449\n",
      "Epoch 773/1024\n",
      "9/9 - 1s - loss: 0.3463\n",
      "Epoch 774/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 775/1024\n",
      "9/9 - 1s - loss: 0.3492\n",
      "Epoch 776/1024\n",
      "9/9 - 1s - loss: 0.3491\n",
      "Epoch 777/1024\n",
      "9/9 - 1s - loss: 0.3520\n",
      "Epoch 778/1024\n",
      "9/9 - 1s - loss: 0.3490\n",
      "Epoch 779/1024\n",
      "9/9 - 1s - loss: 0.3538\n",
      "Epoch 780/1024\n",
      "9/9 - 1s - loss: 0.3487\n",
      "Epoch 781/1024\n",
      "9/9 - 1s - loss: 0.3492\n",
      "Epoch 782/1024\n",
      "9/9 - 1s - loss: 0.3454\n",
      "Epoch 783/1024\n",
      "9/9 - 1s - loss: 0.3477\n",
      "Epoch 784/1024\n",
      "9/9 - 1s - loss: 0.3507\n",
      "Epoch 785/1024\n",
      "9/9 - 1s - loss: 0.3506\n",
      "Epoch 786/1024\n",
      "9/9 - 1s - loss: 0.3454\n",
      "Epoch 787/1024\n",
      "9/9 - 1s - loss: 0.3476\n",
      "Epoch 788/1024\n",
      "9/9 - 1s - loss: 0.3499\n",
      "Epoch 789/1024\n",
      "9/9 - 1s - loss: 0.3487\n",
      "Epoch 790/1024\n",
      "9/9 - 1s - loss: 0.3498\n",
      "Epoch 791/1024\n",
      "9/9 - 1s - loss: 0.3505\n",
      "Epoch 792/1024\n",
      "9/9 - 1s - loss: 0.3496\n",
      "Epoch 793/1024\n",
      "9/9 - 1s - loss: 0.3495\n",
      "Epoch 794/1024\n",
      "9/9 - 1s - loss: 0.3537\n",
      "Epoch 795/1024\n",
      "9/9 - 1s - loss: 0.3556\n",
      "Epoch 796/1024\n",
      "9/9 - 1s - loss: 0.3480\n",
      "Epoch 797/1024\n",
      "9/9 - 1s - loss: 0.3464\n",
      "Epoch 798/1024\n",
      "9/9 - 1s - loss: 0.3518\n",
      "Epoch 799/1024\n",
      "9/9 - 1s - loss: 0.3509\n",
      "Epoch 800/1024\n",
      "9/9 - 1s - loss: 0.3459\n",
      "Epoch 801/1024\n",
      "9/9 - 1s - loss: 0.3504\n",
      "Epoch 802/1024\n",
      "9/9 - 1s - loss: 0.3457\n",
      "Epoch 803/1024\n",
      "9/9 - 1s - loss: 0.3464\n",
      "Epoch 804/1024\n",
      "9/9 - 1s - loss: 0.3470\n",
      "Epoch 805/1024\n",
      "9/9 - 1s - loss: 0.3513\n",
      "Epoch 806/1024\n",
      "9/9 - 1s - loss: 0.3472\n",
      "Epoch 807/1024\n",
      "9/9 - 1s - loss: 0.3457\n",
      "Epoch 808/1024\n",
      "9/9 - 1s - loss: 0.3466\n",
      "Epoch 809/1024\n",
      "9/9 - 1s - loss: 0.3536\n",
      "Epoch 810/1024\n",
      "9/9 - 1s - loss: 0.3467\n",
      "Epoch 811/1024\n",
      "9/9 - 1s - loss: 0.3456\n",
      "Epoch 812/1024\n",
      "9/9 - 1s - loss: 0.3480\n",
      "Epoch 813/1024\n",
      "9/9 - 1s - loss: 0.3471\n",
      "Epoch 814/1024\n",
      "9/9 - 1s - loss: 0.3504\n",
      "Epoch 815/1024\n",
      "9/9 - 1s - loss: 0.3507\n",
      "Epoch 816/1024\n",
      "9/9 - 1s - loss: 0.3435\n",
      "Epoch 817/1024\n",
      "9/9 - 1s - loss: 0.3467\n",
      "Epoch 818/1024\n",
      "9/9 - 1s - loss: 0.3499\n",
      "Epoch 819/1024\n",
      "9/9 - 1s - loss: 0.3531\n",
      "Epoch 820/1024\n",
      "9/9 - 1s - loss: 0.3454\n",
      "Epoch 821/1024\n",
      "9/9 - 1s - loss: 0.3479\n",
      "Epoch 822/1024\n",
      "9/9 - 1s - loss: 0.3462\n",
      "Epoch 823/1024\n",
      "9/9 - 1s - loss: 0.3443\n",
      "Epoch 824/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 825/1024\n",
      "9/9 - 1s - loss: 0.3472\n",
      "Epoch 826/1024\n",
      "9/9 - 1s - loss: 0.3429\n",
      "Epoch 827/1024\n",
      "9/9 - 1s - loss: 0.3487\n",
      "Epoch 828/1024\n",
      "9/9 - 1s - loss: 0.3493\n",
      "Epoch 829/1024\n",
      "9/9 - 1s - loss: 0.3458\n",
      "Epoch 830/1024\n",
      "9/9 - 1s - loss: 0.3445\n",
      "Epoch 831/1024\n",
      "9/9 - 1s - loss: 0.3475\n",
      "Epoch 832/1024\n",
      "9/9 - 1s - loss: 0.3479\n",
      "Epoch 833/1024\n",
      "9/9 - 1s - loss: 0.3446\n",
      "Epoch 834/1024\n",
      "9/9 - 1s - loss: 0.3475\n",
      "Epoch 835/1024\n",
      "9/9 - 1s - loss: 0.3471\n",
      "Epoch 836/1024\n",
      "9/9 - 1s - loss: 0.3436\n",
      "Epoch 837/1024\n",
      "9/9 - 1s - loss: 0.3457\n",
      "Epoch 838/1024\n",
      "9/9 - 1s - loss: 0.3454\n",
      "Epoch 839/1024\n",
      "9/9 - 1s - loss: 0.3466\n",
      "Epoch 840/1024\n",
      "9/9 - 1s - loss: 0.3455\n",
      "Epoch 841/1024\n",
      "9/9 - 1s - loss: 0.3499\n",
      "Epoch 842/1024\n",
      "9/9 - 1s - loss: 0.3479\n",
      "Epoch 843/1024\n",
      "9/9 - 1s - loss: 0.3506\n",
      "Epoch 844/1024\n",
      "9/9 - 1s - loss: 0.3471\n",
      "Epoch 845/1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 1s - loss: 0.3476\n",
      "Epoch 846/1024\n",
      "9/9 - 1s - loss: 0.3482\n",
      "Epoch 847/1024\n",
      "9/9 - 1s - loss: 0.3425\n",
      "Epoch 848/1024\n",
      "9/9 - 1s - loss: 0.3452\n",
      "Epoch 849/1024\n",
      "9/9 - 1s - loss: 0.3505\n",
      "Epoch 850/1024\n",
      "9/9 - 1s - loss: 0.3442\n",
      "Epoch 851/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 852/1024\n",
      "9/9 - 1s - loss: 0.3461\n",
      "Epoch 853/1024\n",
      "9/9 - 1s - loss: 0.3473\n",
      "Epoch 854/1024\n",
      "9/9 - 1s - loss: 0.3417\n",
      "Epoch 855/1024\n",
      "9/9 - 1s - loss: 0.3474\n",
      "Epoch 856/1024\n",
      "9/9 - 1s - loss: 0.3486\n",
      "Epoch 857/1024\n",
      "9/9 - 1s - loss: 0.3493\n",
      "Epoch 858/1024\n",
      "9/9 - 1s - loss: 0.3487\n",
      "Epoch 859/1024\n",
      "9/9 - 1s - loss: 0.3506\n",
      "Epoch 860/1024\n",
      "9/9 - 1s - loss: 0.3486\n",
      "Epoch 861/1024\n",
      "9/9 - 1s - loss: 0.3429\n",
      "Epoch 862/1024\n",
      "9/9 - 1s - loss: 0.3478\n",
      "Epoch 863/1024\n",
      "9/9 - 1s - loss: 0.3411\n",
      "Epoch 864/1024\n",
      "9/9 - 1s - loss: 0.3432\n",
      "Epoch 865/1024\n",
      "9/9 - 1s - loss: 0.3474\n",
      "Epoch 866/1024\n",
      "9/9 - 1s - loss: 0.3461\n",
      "Epoch 867/1024\n",
      "9/9 - 1s - loss: 0.3456\n",
      "Epoch 868/1024\n",
      "9/9 - 1s - loss: 0.3503\n",
      "Epoch 869/1024\n",
      "9/9 - 1s - loss: 0.3447\n",
      "Epoch 870/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 871/1024\n",
      "9/9 - 1s - loss: 0.3512\n",
      "Epoch 872/1024\n",
      "9/9 - 1s - loss: 0.3449\n",
      "Epoch 873/1024\n",
      "9/9 - 1s - loss: 0.3466\n",
      "Epoch 874/1024\n",
      "9/9 - 1s - loss: 0.3442\n",
      "Epoch 875/1024\n",
      "9/9 - 1s - loss: 0.3479\n",
      "Epoch 876/1024\n",
      "9/9 - 1s - loss: 0.3456\n",
      "Epoch 877/1024\n",
      "9/9 - 1s - loss: 0.3437\n",
      "Epoch 878/1024\n",
      "9/9 - 1s - loss: 0.3408\n",
      "Epoch 879/1024\n",
      "9/9 - 1s - loss: 0.3458\n",
      "Epoch 880/1024\n",
      "9/9 - 1s - loss: 0.3453\n",
      "Epoch 881/1024\n",
      "9/9 - 1s - loss: 0.3473\n",
      "Epoch 882/1024\n",
      "9/9 - 1s - loss: 0.3459\n",
      "Epoch 883/1024\n",
      "9/9 - 1s - loss: 0.3465\n",
      "Epoch 884/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 885/1024\n",
      "9/9 - 1s - loss: 0.3414\n",
      "Epoch 886/1024\n",
      "9/9 - 1s - loss: 0.3447\n",
      "Epoch 887/1024\n",
      "9/9 - 1s - loss: 0.3496\n",
      "Epoch 888/1024\n",
      "9/9 - 1s - loss: 0.3460\n",
      "Epoch 889/1024\n",
      "9/9 - 1s - loss: 0.3514\n",
      "Epoch 890/1024\n",
      "9/9 - 1s - loss: 0.3453\n",
      "Epoch 891/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 892/1024\n",
      "9/9 - 1s - loss: 0.3434\n",
      "Epoch 893/1024\n",
      "9/9 - 1s - loss: 0.3497\n",
      "Epoch 894/1024\n",
      "9/9 - 1s - loss: 0.3403\n",
      "Epoch 895/1024\n",
      "9/9 - 1s - loss: 0.3459\n",
      "Epoch 896/1024\n",
      "9/9 - 1s - loss: 0.3452\n",
      "Epoch 897/1024\n",
      "9/9 - 1s - loss: 0.3456\n",
      "Epoch 898/1024\n",
      "9/9 - 1s - loss: 0.3486\n",
      "Epoch 899/1024\n",
      "9/9 - 1s - loss: 0.3423\n",
      "Epoch 900/1024\n",
      "9/9 - 1s - loss: 0.3404\n",
      "Epoch 901/1024\n",
      "9/9 - 1s - loss: 0.3459\n",
      "Epoch 902/1024\n",
      "9/9 - 1s - loss: 0.3465\n",
      "Epoch 903/1024\n",
      "9/9 - 1s - loss: 0.3468\n",
      "Epoch 904/1024\n",
      "9/9 - 1s - loss: 0.3443\n",
      "Epoch 905/1024\n",
      "9/9 - 1s - loss: 0.3471\n",
      "Epoch 906/1024\n",
      "9/9 - 1s - loss: 0.3453\n",
      "Epoch 907/1024\n",
      "9/9 - 1s - loss: 0.3426\n",
      "Epoch 908/1024\n",
      "9/9 - 1s - loss: 0.3407\n",
      "Epoch 909/1024\n",
      "9/9 - 1s - loss: 0.3478\n",
      "Epoch 910/1024\n",
      "9/9 - 1s - loss: 0.3520\n",
      "Epoch 911/1024\n",
      "9/9 - 1s - loss: 0.3448\n",
      "Epoch 912/1024\n",
      "9/9 - 1s - loss: 0.3490\n",
      "Epoch 913/1024\n",
      "9/9 - 1s - loss: 0.3441\n",
      "Epoch 914/1024\n",
      "9/9 - 1s - loss: 0.3438\n",
      "Epoch 915/1024\n",
      "9/9 - 1s - loss: 0.3489\n",
      "Epoch 916/1024\n",
      "9/9 - 1s - loss: 0.3416\n",
      "Epoch 917/1024\n",
      "9/9 - 1s - loss: 0.3409\n",
      "Epoch 918/1024\n",
      "9/9 - 1s - loss: 0.3448\n",
      "Epoch 919/1024\n",
      "9/9 - 1s - loss: 0.3514\n",
      "Epoch 920/1024\n",
      "9/9 - 1s - loss: 0.3473\n",
      "Epoch 921/1024\n",
      "9/9 - 1s - loss: 0.3467\n",
      "Epoch 922/1024\n",
      "9/9 - 1s - loss: 0.3470\n",
      "Epoch 923/1024\n",
      "9/9 - 1s - loss: 0.3476\n",
      "Epoch 924/1024\n",
      "9/9 - 1s - loss: 0.3440\n",
      "Epoch 925/1024\n",
      "9/9 - 1s - loss: 0.3412\n",
      "Epoch 926/1024\n",
      "9/9 - 1s - loss: 0.3405\n",
      "Epoch 927/1024\n",
      "9/9 - 1s - loss: 0.3454\n",
      "Epoch 928/1024\n",
      "9/9 - 1s - loss: 0.3462\n",
      "Epoch 929/1024\n",
      "9/9 - 1s - loss: 0.3432\n",
      "Epoch 930/1024\n",
      "9/9 - 1s - loss: 0.3469\n",
      "Epoch 931/1024\n",
      "9/9 - 1s - loss: 0.3449\n",
      "Epoch 932/1024\n",
      "9/9 - 1s - loss: 0.3411\n",
      "Epoch 933/1024\n",
      "9/9 - 1s - loss: 0.3462\n",
      "Epoch 934/1024\n",
      "9/9 - 1s - loss: 0.3442\n",
      "Epoch 935/1024\n",
      "9/9 - 1s - loss: 0.3459\n",
      "Epoch 936/1024\n",
      "9/9 - 1s - loss: 0.3470\n",
      "Epoch 937/1024\n",
      "9/9 - 1s - loss: 0.3476\n",
      "Epoch 938/1024\n",
      "9/9 - 1s - loss: 0.3451\n",
      "Epoch 939/1024\n",
      "9/9 - 1s - loss: 0.3468\n",
      "Epoch 940/1024\n",
      "9/9 - 1s - loss: 0.3447\n",
      "Epoch 941/1024\n",
      "9/9 - 1s - loss: 0.3476\n",
      "Epoch 942/1024\n",
      "9/9 - 1s - loss: 0.3432\n",
      "Epoch 943/1024\n",
      "9/9 - 1s - loss: 0.3485\n",
      "Epoch 944/1024\n",
      "9/9 - 1s - loss: 0.3448\n",
      "Epoch 945/1024\n",
      "9/9 - 1s - loss: 0.3441\n",
      "Epoch 946/1024\n",
      "9/9 - 1s - loss: 0.3444\n",
      "Epoch 947/1024\n",
      "9/9 - 1s - loss: 0.3428\n",
      "Epoch 948/1024\n",
      "9/9 - 1s - loss: 0.3467\n",
      "Epoch 949/1024\n",
      "9/9 - 1s - loss: 0.3404\n",
      "Epoch 950/1024\n",
      "9/9 - 1s - loss: 0.3459\n",
      "Epoch 951/1024\n",
      "9/9 - 1s - loss: 0.3416\n",
      "Epoch 952/1024\n",
      "9/9 - 1s - loss: 0.3446\n",
      "Epoch 953/1024\n",
      "9/9 - 1s - loss: 0.3542\n",
      "Epoch 954/1024\n",
      "9/9 - 1s - loss: 0.3422\n",
      "Epoch 955/1024\n",
      "9/9 - 1s - loss: 0.3459\n",
      "Epoch 956/1024\n",
      "9/9 - 1s - loss: 0.3463\n",
      "Epoch 957/1024\n",
      "9/9 - 1s - loss: 0.3446\n",
      "Epoch 958/1024\n",
      "9/9 - 1s - loss: 0.3448\n",
      "Epoch 959/1024\n",
      "9/9 - 1s - loss: 0.3485\n",
      "Epoch 960/1024\n",
      "9/9 - 1s - loss: 0.3487\n",
      "Epoch 961/1024\n",
      "9/9 - 1s - loss: 0.3477\n",
      "Epoch 962/1024\n",
      "9/9 - 1s - loss: 0.3449\n",
      "Epoch 963/1024\n",
      "9/9 - 1s - loss: 0.3422\n",
      "Epoch 964/1024\n",
      "9/9 - 1s - loss: 0.3490\n",
      "Epoch 965/1024\n",
      "9/9 - 1s - loss: 0.3442\n",
      "Epoch 966/1024\n",
      "9/9 - 1s - loss: 0.3460\n",
      "Epoch 967/1024\n",
      "9/9 - 1s - loss: 0.3478\n",
      "Epoch 968/1024\n",
      "9/9 - 1s - loss: 0.3494\n",
      "Epoch 969/1024\n",
      "9/9 - 1s - loss: 0.3499\n",
      "Epoch 970/1024\n",
      "9/9 - 1s - loss: 0.3464\n",
      "Epoch 971/1024\n",
      "9/9 - 1s - loss: 0.3432\n",
      "Epoch 972/1024\n",
      "9/9 - 1s - loss: 0.3479\n",
      "Epoch 973/1024\n",
      "9/9 - 1s - loss: 0.3475\n",
      "Epoch 974/1024\n",
      "9/9 - 1s - loss: 0.3450\n",
      "Epoch 975/1024\n",
      "9/9 - 1s - loss: 0.3408\n",
      "Epoch 976/1024\n",
      "9/9 - 1s - loss: 0.3445\n",
      "Epoch 977/1024\n",
      "9/9 - 1s - loss: 0.3440\n",
      "Epoch 978/1024\n",
      "9/9 - 1s - loss: 0.3459\n",
      "Epoch 979/1024\n",
      "9/9 - 1s - loss: 0.3437\n",
      "Epoch 980/1024\n",
      "9/9 - 1s - loss: 0.3487\n",
      "Epoch 981/1024\n",
      "9/9 - 1s - loss: 0.3426\n",
      "Epoch 982/1024\n",
      "9/9 - 1s - loss: 0.3462\n",
      "Epoch 983/1024\n",
      "9/9 - 1s - loss: 0.3447\n",
      "Epoch 984/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 985/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 986/1024\n",
      "9/9 - 1s - loss: 0.3479\n",
      "Epoch 987/1024\n",
      "9/9 - 1s - loss: 0.3460\n",
      "Epoch 988/1024\n",
      "9/9 - 1s - loss: 0.3458\n",
      "Epoch 989/1024\n",
      "9/9 - 1s - loss: 0.3494\n",
      "Epoch 990/1024\n",
      "9/9 - 1s - loss: 0.3464\n",
      "Epoch 991/1024\n",
      "9/9 - 1s - loss: 0.3515\n",
      "Epoch 992/1024\n",
      "9/9 - 1s - loss: 0.3474\n",
      "Epoch 993/1024\n",
      "9/9 - 1s - loss: 0.3481\n",
      "Epoch 994/1024\n",
      "9/9 - 1s - loss: 0.3476\n",
      "Epoch 995/1024\n",
      "9/9 - 1s - loss: 0.3456\n",
      "Epoch 996/1024\n",
      "9/9 - 1s - loss: 0.3452\n",
      "Epoch 997/1024\n",
      "9/9 - 1s - loss: 0.3455\n",
      "Epoch 998/1024\n",
      "9/9 - 1s - loss: 0.3477\n",
      "Epoch 999/1024\n",
      "9/9 - 1s - loss: 0.3516\n",
      "Epoch 1000/1024\n",
      "9/9 - 1s - loss: 0.3467\n",
      "Epoch 1001/1024\n",
      "9/9 - 1s - loss: 0.3522\n",
      "Epoch 1002/1024\n",
      "9/9 - 1s - loss: 0.3448\n",
      "Epoch 1003/1024\n",
      "9/9 - 1s - loss: 0.3442\n",
      "Epoch 1004/1024\n",
      "9/9 - 1s - loss: 0.3466\n",
      "Epoch 1005/1024\n",
      "9/9 - 1s - loss: 0.3478\n",
      "Epoch 1006/1024\n",
      "9/9 - 1s - loss: 0.3491\n",
      "Epoch 1007/1024\n",
      "9/9 - 1s - loss: 0.3450\n",
      "Epoch 1008/1024\n",
      "9/9 - 1s - loss: 0.3490\n",
      "Epoch 1009/1024\n",
      "9/9 - 1s - loss: 0.3432\n",
      "Epoch 1010/1024\n",
      "9/9 - 1s - loss: 0.3482\n",
      "Epoch 1011/1024\n",
      "9/9 - 1s - loss: 0.3456\n",
      "Epoch 1012/1024\n",
      "9/9 - 1s - loss: 0.3419\n",
      "Epoch 1013/1024\n",
      "9/9 - 1s - loss: 0.3446\n",
      "Epoch 1014/1024\n",
      "9/9 - 1s - loss: 0.3468\n",
      "Epoch 1015/1024\n",
      "9/9 - 1s - loss: 0.3446\n",
      "Epoch 1016/1024\n",
      "9/9 - 1s - loss: 0.3486\n",
      "Epoch 1017/1024\n",
      "9/9 - 1s - loss: 0.3462\n",
      "Epoch 1018/1024\n",
      "9/9 - 1s - loss: 0.3446\n",
      "Epoch 1019/1024\n",
      "9/9 - 1s - loss: 0.3454\n",
      "Epoch 1020/1024\n",
      "9/9 - 1s - loss: 0.3465\n",
      "Epoch 1021/1024\n",
      "9/9 - 1s - loss: 0.3491\n",
      "Epoch 1022/1024\n",
      "9/9 - 1s - loss: 0.3397\n",
      "Epoch 1023/1024\n",
      "9/9 - 1s - loss: 0.3480\n",
      "Epoch 1024/1024\n",
      "9/9 - 1s - loss: 0.3503\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: custom_word_2_vec/assets\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_RETRAIN_WORD2VEC:\n",
    "    # Configure data\n",
    "    dataset_size_limt = 100000 #len(df_data)#10000\n",
    "    df_data = df_data[:dataset_size_limt]\n",
    "    test_subset = df_data[:int(len(df_data)*0.2)]\n",
    "    train_subset = df_data[int(len(df_data)*0.2):]\n",
    "    \n",
    "    # Fit the word2vec model\n",
    "    training_tokens = np.array([x.split() for x in train_subset[\"title\"]])\n",
    "    w2v_custom = Word2VecModel()\n",
    "    w2v_custom.fit(training_tokens, epochs=1024, batch_size=8192)\n",
    "    \n",
    "    # Save the word 2 vec model\n",
    "    w2v_custom.save(WORD2VEC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "c = Classifier(WORD2VEC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.sample(frac=1).reset_index(drop=True)\n",
    "classes = np.array(df_data[\"categories\"])\n",
    "classes = classes[:10000]\n",
    "titles = np.array(df_data[\"title\"])\n",
    "titles = titles[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.27450184964364355 Precision 0.34790050327571603 Recall 0.27403180081145845 Accuracy 0.583\n",
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.29567738013005757 Precision 0.36602474587699096 Recall 0.28797188758191894 Accuracy 0.557\n",
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.2886431104252403 Precision 0.36912531484244576 Recall 0.27967212138062003 Accuracy 0.613\n",
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.2854129643546249 Precision 0.31232194207479 Recall 0.29859429247121233 Accuracy 0.59\n",
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.2882814629066981 Precision 0.3439612984921832 Recall 0.2847974324573801 Accuracy 0.612\n",
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.26008668281444236 Precision 0.3201366642357935 Recall 0.26834052338472186 Accuracy 0.568\n",
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.2432699193629654 Precision 0.29999803503411815 Recall 0.23974484212274716 Accuracy 0.598\n",
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.3050888605049027 Precision 0.40337939475513135 Recall 0.30127047496506065 Accuracy 0.586\n",
      "FOLD  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ruchtia/git/word2vec/word2vec/classifier.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  word_embeddings = np.array(word_embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Score: 0.27957341336615193 Precision 0.3340061710621563 Recall 0.2717972217523461 Accuracy 0.585\n",
      "FOLD  0\n",
      "F Score: 0.29957498379039815 Precision 0.3582452949231148 Recall 0.2993681106605503 Accuracy 0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(titles):\n",
    "    print(\"FOLD \", i)\n",
    "    X_train, X_test = titles[train_index], titles[test_index]\n",
    "    y_train, y_test = classes[train_index], classes[test_index]\n",
    "    c.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = c.predict(X_test)\n",
    "    f_score = sklearn.metrics.f1_score(y_test, predictions, average='macro')\n",
    "    precision = sklearn.metrics.precision_score(y_test, predictions, average='macro')\n",
    "    recall = sklearn.metrics.recall_score(y_test, predictions, average='macro')\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, predictions)\n",
    "    print(\"F Score:\", f_score, \"Precision\", precision, \"Recall\", recall, \"Accuracy\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
